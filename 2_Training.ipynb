{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import lightning as L\n",
    "import pytorch_lightning as pl\n",
    "from fastdtw import fastdtw\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({\n",
    "    'font.family':'Times New Roman', \n",
    "    'font.size': 14, \n",
    "    'axes.titlesize': 16, \n",
    "    'axes.labelsize': 14, \n",
    "    'xtick.labelsize': 12, \n",
    "    'ytick.labelsize': 12,\n",
    "    'legend.fontsize': 14, \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_from_file(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "train_A = load_from_file('dataset/train_A.pkl')\n",
    "valid_A = load_from_file('dataset/valid_A.pkl')\n",
    "test_A = load_from_file('dataset/test_A.pkl')\n",
    "\n",
    "train_B = load_from_file('dataset/train_B.pkl')\n",
    "valid_B = load_from_file('dataset/valid_B.pkl')\n",
    "test_B = load_from_file('dataset/test_B.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_subsequences(data_list, sub_length=288, ratio=0.75):\n",
    "\n",
    "    all_subsequences = []\n",
    "\n",
    "    for time_series in data_list:\n",
    "        step = int(sub_length * (1 - ratio))\n",
    "        num_subsequences = (len(time_series) - sub_length) // step + 1\n",
    "\n",
    "        for i in range(int(num_subsequences)):\n",
    "            start_index = int(i * step)\n",
    "            end_index = start_index + sub_length\n",
    "            \n",
    "            if end_index > len(time_series):\n",
    "                break\n",
    "            \n",
    "            subsequence = time_series[start_index:end_index]\n",
    "            all_subsequences.append(subsequence)\n",
    "    return np.array(all_subsequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(data, batch_size, sub_length=288, ratio=0.5, shuffle=True):\n",
    "    array = generate_subsequences(data, sub_length, ratio)\n",
    "\n",
    "    tensor = torch.tensor(array, dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "    dataset = TensorDataset(tensor)\n",
    "\n",
    "    dataloader = DataLoader(dataset, \n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=shuffle, \n",
    "                            # num_workers=2  \n",
    "                            )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "sub_length = 288\n",
    "ratio = 0.75\n",
    "\n",
    "train_A_loader = get_loader(train_A, batch_size, sub_length, ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in train_A_loader:\n",
    "    print(data[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = 2 * embedding_dim\n",
    "\n",
    "        self.gru1 = nn.GRU(\n",
    "            input_size=self.input_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True,  \n",
    "        )\n",
    "\n",
    "        self.gru2 = nn.GRU(\n",
    "            input_size=self.hidden_dim,\n",
    "            hidden_size=embedding_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]  \n",
    "        x, h_n = self.gru1(x)\n",
    "        x, h_n = self.gru2(x)\n",
    "        return h_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_2(nn.Module):\n",
    "    def __init__(self, seq_length, input_dim, embedding_dim):\n",
    "        super(Decoder_2, self).__init__()\n",
    "        self.seq_length = seq_length\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim  = 2 * embedding_dim\n",
    "        \n",
    "        self.gru1 = nn.GRU(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=embedding_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.gru2 = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Linear(self.hidden_dim,\n",
    "                                      self.input_dim)\n",
    "\n",
    "    def forward(self, xh, targets=None, teacher_forcing_ratio=0):\n",
    "        batch_size = xh.size(1)\n",
    "\n",
    "        if targets is not None:\n",
    "            decoder_input = targets[:, 0:1, :] \n",
    "        else:\n",
    "            decoder_input = torch.zeros(batch_size, 1, self.input_dim, device=xh.device)\n",
    "\n",
    "        decoder_hidden1 = xh  \n",
    "        decoder_hidden2 = torch.zeros(1, batch_size, self.hidden_dim, device=xh.device)  \n",
    "\n",
    "        outputs = []\n",
    "        seq_len = self.seq_length\n",
    "        for t in range(seq_len):\n",
    "            out, decoder_hidden1 = self.gru1(decoder_input, decoder_hidden1)\n",
    "            out, decoder_hidden2  = self.gru2(out, decoder_hidden2)\n",
    "\n",
    "            out = self.output_layer(out)\n",
    "            outputs.append(out)\n",
    "\n",
    "            if targets is not None and torch.rand(1).item() < teacher_forcing_ratio:\n",
    "                decoder_input = targets[:, t:t+1, :]\n",
    "            else:\n",
    "                # decoder_input = out\n",
    "                decoder_input = out.detach()  \n",
    "\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dtw_loss(y_true, y_pred):\n",
    "#     y_true_np = y_true.detach().cpu().numpy()\n",
    "#     y_pred_np = y_pred.detach().cpu().numpy()\n",
    "#     total_dtw_distance = 0\n",
    "    \n",
    "#     for true, pred in zip(y_true_np, y_pred_np):\n",
    "#         distance, _ = fastdtw(true, pred) \n",
    "#         total_dtw_distance += distance\n",
    "    \n",
    "#     return torch.tensor(total_dtw_distance / y_true.size(0), requires_grad=True)\n",
    "\n",
    "# class CombinedDTW_MSE_Loss(nn.Module):\n",
    "#     def __init__(self, alpha=0.5):\n",
    "#         super(CombinedDTW_MSE_Loss, self).__init__()\n",
    "#         self.alpha = alpha  \n",
    "#         self.mse_loss = nn.MSELoss()  \n",
    "\n",
    "#     def forward(self, y_true, y_pred):\n",
    "#         mse = self.mse_loss(y_true, y_pred)\n",
    "#         dtw = dtw_loss(y_true, y_pred)\n",
    "#         combined_loss = self.alpha * mse + (1 - self.alpha) * dtw\n",
    "#         return combined_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fluctuation_sensitive_loss(y_true, y_pred):\n",
    "    delta_true = y_true[:, 1:] - y_true[:, :-1]  \n",
    "    delta_pred = y_pred[:, 1:] - y_pred[:, :-1]  \n",
    "    loss = torch.mean((delta_true - delta_pred) ** 2).to(y_true.device)\n",
    "    return loss\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, y_true, y_pred):\n",
    "        mse = self.mse_loss(y_true, y_pred)\n",
    "        fluctuation_loss = fluctuation_sensitive_loss(y_true, y_pred)\n",
    "        combined_loss = self.alpha * mse + (1 - self.alpha) * fluctuation_loss\n",
    "        return combined_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoencoderGRU_2(pl.LightningModule):\n",
    "    def __init__(self, seq_length, input_dim, embedding_dim, teacher_forcing_ratio=0, alpha=0.5):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_dim, embedding_dim)\n",
    "        self.decoder = Decoder_2(seq_length, input_dim, embedding_dim)\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "        self.loss_fn = CombinedLoss(alpha=alpha)\n",
    "    \n",
    "    def forward(self, x, teacher_forcing_ratio=None):\n",
    "        if teacher_forcing_ratio is None:\n",
    "            teacher_forcing_ratio = self.teacher_forcing_ratio\n",
    "        xh = self.encoder(x)\n",
    "        out = self.decoder(xh, targets=x, teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "        return out\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch[0]\n",
    "        pred = self(x)\n",
    "        loss = self.loss_fn(pred, x)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch[0]\n",
    "        pred = self(x, teacher_forcing_ratio=0.0)\n",
    "        loss = self.loss_fn(pred, x)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch[0]\n",
    "        pred = self(x, teacher_forcing_ratio=0.0)\n",
    "        loss = self.loss_fn(pred, x)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x = batch[0]\n",
    "        pred = self(x, teacher_forcing_ratio=0.0)\n",
    "        return pred\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        train_loss = self.trainer.callback_metrics.get('train_loss')\n",
    "        if train_loss is not None:\n",
    "            self.train_losses.append(train_loss.cpu().detach().item())\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        val_loss = self.trainer.callback_metrics.get('val_loss')\n",
    "        if val_loss is not None:\n",
    "            self.val_losses.append(val_loss.cpu().detach().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "# from pytorch_lightning.loggers import WandbLogger\n",
    "# from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "class PlotLossesCallback(pl.Callback):\n",
    "    def __init__(self, name=\"ex_id\"):\n",
    "        self.name = name\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        plt.figure(figsize=(8, 5)) \n",
    "        plt.plot(pl_module.train_losses, label='Training Loss', linestyle='-', color='b')\n",
    "        if len(pl_module.val_losses) > 1: \n",
    "            plt.plot(pl_module.val_losses[1:], label='Validation Loss', linestyle='--', color='r')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "\n",
    "        plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "        plt.legend()\n",
    "        plt.title('Training and Validation Loss During Model Training')\n",
    "        plt.grid(True)  \n",
    "        plt.savefig(f'{self.name} loss.svg', dpi=300) \n",
    "        plt.show()\n",
    "\n",
    "def get_trainer(name=\"ex_id\"):\n",
    "    # tf_logger = TensorBoardLogger(\"tb_logs\", name=name)\n",
    "    # wandb_logger = WandbLogger(project=name)\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_loss',             \n",
    "        dirpath=f'checkpoint/{name}',  \n",
    "        filename='{epoch:02d}-{val_loss:.2f}', \n",
    "        save_top_k=3, \n",
    "        mode='min'\n",
    "    )\n",
    "\n",
    "    early_stopping_callback = EarlyStopping(\n",
    "        monitor='val_loss',            \n",
    "        patience=5,       \n",
    "        mode='min',  \n",
    "        verbose=True  \n",
    "    )\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=100,\n",
    "        log_every_n_steps=1,\n",
    "        callbacks=[\n",
    "            checkpoint_callback, \n",
    "            # early_stopping_callback, \n",
    "            PlotLossesCallback(name=name)\n",
    "            ],\n",
    "        # logger = wandb_logger\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_A_loader_288 = get_loader(data=train_A, batch_size=32, sub_length=288, ratio=0.75)\n",
    "train_B_loader_288 = get_loader(data=train_B, batch_size=32, sub_length=288, ratio=0.75)\n",
    "\n",
    "val_A_loader_288 = get_loader(data=valid_A, batch_size=32, sub_length=288, ratio=0.75, shuffle=False)\n",
    "val_B_loader_288 = get_loader(data=valid_B, batch_size=32, sub_length=288, ratio=0.75, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length=288\n",
    "embedding_dim = int(seq_length / 2) \n",
    "teacher_forcing_ratio = 0 \n",
    "\n",
    "model1_1 = AutoencoderGRU_2(\n",
    "    seq_length=seq_length,\n",
    "    input_dim=1,\n",
    "    embedding_dim=embedding_dim,\n",
    "    teacher_forcing_ratio=teacher_forcing_ratio\n",
    ")\n",
    "\n",
    "trainer1_1 = get_trainer(\"ex1_1\")\n",
    "trainer1_1.fit(model1_1, train_A_loader_288, val_A_loader_288)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length=288\n",
    "embedding_dim = int(seq_length / 2)\n",
    "teacher_forcing_ratio = 0.25\n",
    "\n",
    "model1_2 = AutoencoderGRU_2(\n",
    "    seq_length=seq_length,\n",
    "    input_dim=1,\n",
    "    embedding_dim=embedding_dim,\n",
    "    teacher_forcing_ratio=teacher_forcing_ratio\n",
    ")\n",
    "\n",
    "trainer1_2 = get_trainer(\"ex1_2\")\n",
    "trainer1_2.fit(model1_2, train_A_loader_288, val_A_loader_288)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length=288\n",
    "embedding_dim = int(seq_length / 2)\n",
    "teacher_forcing_ratio = 0.5 \n",
    "\n",
    "model1_3 = AutoencoderGRU_2(\n",
    "    seq_length=seq_length,\n",
    "    input_dim=1,\n",
    "    embedding_dim=embedding_dim,\n",
    "    teacher_forcing_ratio=teacher_forcing_ratio\n",
    ")\n",
    "\n",
    "trainer1_3 = get_trainer(\"ex1_3\")\n",
    "trainer1_3.fit(model1_3, train_A_loader_288, val_A_loader_288)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length=288\n",
    "embedding_dim = int(seq_length / 2) \n",
    "teacher_forcing_ratio = 0.75 \n",
    "\n",
    "model1_4 = AutoencoderGRU_2(\n",
    "    seq_length=seq_length,\n",
    "    input_dim=1,\n",
    "    embedding_dim=embedding_dim,\n",
    "    teacher_forcing_ratio=teacher_forcing_ratio\n",
    ")\n",
    "\n",
    "trainer1_4 = get_trainer(\"ex1_4\")\n",
    "trainer1_4.fit(model1_4, train_A_loader_288, val_A_loader_288)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length=288\n",
    "embedding_dim = int(seq_length / 2)\n",
    "teacher_forcing_ratio = 1\n",
    "\n",
    "model1_5 = AutoencoderGRU_2(\n",
    "    seq_length=seq_length,\n",
    "    input_dim=1,\n",
    "    embedding_dim=embedding_dim,\n",
    "    teacher_forcing_ratio=teacher_forcing_ratio\n",
    ")\n",
    "\n",
    "trainer1_5 = get_trainer(\"ex1_5\")\n",
    "trainer1_5.fit(model1_5, train_A_loader_288, val_A_loader_288)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length=288\n",
    "embedding_dim = int(seq_length / 2)\n",
    "teacher_forcing_ratio = 0 \n",
    "\n",
    "model1_6 = AutoencoderGRU_2(\n",
    "    seq_length=seq_length,\n",
    "    input_dim=1,\n",
    "    embedding_dim=embedding_dim,\n",
    "    teacher_forcing_ratio=teacher_forcing_ratio\n",
    ")\n",
    "\n",
    "trainer1_6 = get_trainer(\"ex1_6\")\n",
    "trainer1_6.fit(model1_6, train_B_loader_288, val_B_loader_288)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length=288\n",
    "embedding_dim = int(seq_length / 2) \n",
    "teacher_forcing_ratio = 0.25 \n",
    "\n",
    "model1_7 = AutoencoderGRU_2(\n",
    "    seq_length=seq_length,\n",
    "    input_dim=1,\n",
    "    embedding_dim=embedding_dim,\n",
    "    teacher_forcing_ratio=teacher_forcing_ratio\n",
    ")\n",
    "\n",
    "trainer1_7 = get_trainer(\"ex1_7\")\n",
    "trainer1_7.fit(model1_7, train_B_loader_288, val_B_loader_288)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length=288\n",
    "embedding_dim = int(seq_length / 2) \n",
    "teacher_forcing_ratio = 0.5 \n",
    "\n",
    "model1_8 = AutoencoderGRU_2(\n",
    "    seq_length=seq_length,\n",
    "    input_dim=1,\n",
    "    embedding_dim=embedding_dim,\n",
    "    teacher_forcing_ratio=teacher_forcing_ratio\n",
    ")\n",
    "\n",
    "trainer1_8 = get_trainer(\"ex1_8\")\n",
    "trainer1_8.fit(model1_8, train_B_loader_288, val_B_loader_288)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length=288\n",
    "embedding_dim = int(seq_length / 2)\n",
    "teacher_forcing_ratio = 0.75 \n",
    "\n",
    "model1_9 = AutoencoderGRU_2(\n",
    "    seq_length=seq_length,\n",
    "    input_dim=1,\n",
    "    embedding_dim=embedding_dim,\n",
    "    teacher_forcing_ratio=teacher_forcing_ratio\n",
    ")\n",
    "\n",
    "trainer1_9 = get_trainer(\"ex1_9\")\n",
    "trainer1_9.fit(model1_9, train_B_loader_288, val_B_loader_288)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length=288\n",
    "embedding_dim = int(seq_length / 2)\n",
    "teacher_forcing_ratio = 0.9\n",
    "\n",
    "model1_10 = AutoencoderGRU_2(\n",
    "    seq_length=seq_length,\n",
    "    input_dim=1,\n",
    "    embedding_dim=embedding_dim,\n",
    "    teacher_forcing_ratio=teacher_forcing_ratio\n",
    ")\n",
    "\n",
    "trainer1_10 = get_trainer(\"ex1_10\")\n",
    "trainer1_10.fit(model1_10, train_B_loader_288, val_B_loader_288)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dump loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_1_loss = [model1_1.train_losses, model1_1.val_losses,\n",
    "                     model1_2.train_losses, model1_2.val_losses,\n",
    "                     model1_3.train_losses, model1_3.val_losses,\n",
    "                     model1_4.train_losses, model1_4.val_losses,\n",
    "                     model1_5.train_losses, model1_5.val_losses,\n",
    "                     model1_6.train_losses, model1_6.val_losses,\n",
    "                     model1_7.train_losses, model1_7.val_losses,\n",
    "                     model1_8.train_losses, model1_8.val_losses,\n",
    "                     model1_9.train_losses, model1_9.val_losses,\n",
    "                     model1_10.train_losses, model1_10.val_losses]\n",
    "\n",
    "with open('experiment_1_loss.pkl', 'wb') as f:\n",
    "    pickle.dump(experiment_1_loss, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_A_loader_288 = get_loader(data=train_A, batch_size=32, sub_length=288, ratio=0.75)\n",
    "train_B_loader_288 = get_loader(data=train_B, batch_size=32, sub_length=288, ratio=0.75)\n",
    "\n",
    "val_A_loader_288 = get_loader(data=valid_A, batch_size=32, sub_length=288, ratio=0.75, shuffle=False)\n",
    "val_B_loader_288 = get_loader(data=valid_B, batch_size=32, sub_length=288, ratio=0.75, shuffle=False)\n",
    "\n",
    "train_A_loader_144 = get_loader(data=train_A, batch_size=32, sub_length=144, ratio=0.75)\n",
    "train_B_loader_144 = get_loader(data=train_B, batch_size=32, sub_length=144, ratio=0.75)\n",
    "\n",
    "val_A_loader_144 = get_loader(data=valid_A, batch_size=32, sub_length=144, ratio=0.75, shuffle=False)\n",
    "val_B_loader_144 = get_loader(data=valid_B, batch_size=32, sub_length=144, ratio=0.75, shuffle=False)\n",
    "\n",
    "train_A_loader_216 = get_loader(data=train_A, batch_size=32, sub_length=216, ratio=0.75)\n",
    "train_B_loader_216 = get_loader(data=train_B, batch_size=32, sub_length=216, ratio=0.75)\n",
    "\n",
    "val_A_loader_216 = get_loader(data=valid_A, batch_size=32, sub_length=216, ratio=0.75, shuffle=False)\n",
    "val_B_loader_216 = get_loader(data=valid_B, batch_size=32, sub_length=216, ratio=0.75, shuffle=False)\n",
    "\n",
    "train_A_loader_360 = get_loader(data=train_A, batch_size=32, sub_length=360, ratio=0.75)\n",
    "train_B_loader_360 = get_loader(data=train_B, batch_size=32, sub_length=360, ratio=0.75)\n",
    "\n",
    "val_A_loader_360 = get_loader(data=valid_A, batch_size=32, sub_length=360, ratio=0.75, shuffle=False)\n",
    "val_B_loader_360 = get_loader(data=valid_B, batch_size=32, sub_length=360, ratio=0.75, shuffle=False)\n",
    "\n",
    "train_A_loader_432 = get_loader(data=train_A, batch_size=32, sub_length=432, ratio=0.75)\n",
    "train_B_loader_432 = get_loader(data=train_B, batch_size=32, sub_length=432, ratio=0.75)\n",
    "\n",
    "val_A_loader_432 = get_loader(data=valid_A, batch_size=32, sub_length=432, ratio=0.75, shuffle=False)\n",
    "val_B_loader_432 = get_loader(data=valid_B, batch_size=32, sub_length=432, ratio=0.75, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length=144\n",
    "embedding_dim = int(seq_length / 2)\n",
    "teacher_forcing_ratio = 0 \n",
    "\n",
    "model2_1 = AutoencoderGRU_2(\n",
    "    seq_length=seq_length,\n",
    "    input_dim=1,\n",
    "    embedding_dim=embedding_dim,\n",
    "    teacher_forcing_ratio=teacher_forcing_ratio,\n",
    "    alpha=0.5\n",
    ")\n",
    "\n",
    "trainer2_1 = get_trainer(\"ex2_1\")\n",
    "trainer2_1.fit(model2_1, train_A_loader_144, val_A_loader_144)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 216\n",
    "embedding_dim = int(seq_length / 2) \n",
    "teacher_forcing_ratio = 0\n",
    "\n",
    "model2_2 = AutoencoderGRU_2(\n",
    "    seq_length=seq_length,\n",
    "    input_dim=1,\n",
    "    embedding_dim=embedding_dim,\n",
    "    teacher_forcing_ratio=teacher_forcing_ratio\n",
    ")\n",
    "\n",
    "trainer2_2 = get_trainer(\"ex2_2\")\n",
    "trainer2_2.fit(model2_2, train_A_loader_216, val_A_loader_216)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 288\n",
    "embedding_dim = int(seq_length / 2) \n",
    "teacher_forcing_ratio = 0 \n",
    "\n",
    "model2_3 = AutoencoderGRU_2(\n",
    "    seq_length=seq_length,\n",
    "    input_dim=1,\n",
    "    embedding_dim=embedding_dim,\n",
    "    teacher_forcing_ratio=teacher_forcing_ratio\n",
    ")\n",
    "\n",
    "trainer2_3 = get_trainer(\"ex2_3\")\n",
    "trainer2_3.fit(model2_3, train_A_loader_288, val_A_loader_288)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 360\n",
    "embedding_dim = int(seq_length / 2) \n",
    "teacher_forcing_ratio = 0 \n",
    "\n",
    "model2_4 = AutoencoderGRU_2(\n",
    "    seq_length=seq_length,\n",
    "    input_dim=1,\n",
    "    embedding_dim=embedding_dim,\n",
    "    teacher_forcing_ratio=teacher_forcing_ratio\n",
    ")\n",
    "\n",
    "trainer2_4 = get_trainer(\"ex2_4\")\n",
    "trainer2_4.fit(model2_4, train_A_loader_360, val_A_loader_360)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 432\n",
    "embedding_dim = int(seq_length / 2) \n",
    "teacher_forcing_ratio = 0 \n",
    "\n",
    "model2_5 = AutoencoderGRU_2(\n",
    "    seq_length=seq_length,\n",
    "    input_dim=1,\n",
    "    embedding_dim=embedding_dim,\n",
    "    teacher_forcing_ratio=teacher_forcing_ratio\n",
    ")\n",
    "\n",
    "trainer2_5 = get_trainer(\"ex2_5\")\n",
    "trainer2_5.fit(model2_5, train_A_loader_432, val_A_loader_432)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 144\n",
    "embedding_dim = int(seq_length / 2)\n",
    "teacher_forcing_ratio = 0\n",
    "\n",
    "model2_6 = AutoencoderGRU_2(\n",
    "    seq_length=seq_length,\n",
    "    input_dim=1,\n",
    "    embedding_dim=embedding_dim,\n",
    "    teacher_forcing_ratio=teacher_forcing_ratio\n",
    ")\n",
    "\n",
    "trainer2_6 = get_trainer(\"ex2_6\")\n",
    "trainer2_6.fit(model2_6, train_B_loader_144, val_B_loader_144)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 216\n",
    "embedding_dim = int(seq_length / 2)\n",
    "teacher_forcing_ratio = 0\n",
    "\n",
    "model2_7 = AutoencoderGRU_2(\n",
    "    seq_length=seq_length,\n",
    "    input_dim=1,\n",
    "    embedding_dim=embedding_dim,\n",
    "    teacher_forcing_ratio=teacher_forcing_ratio\n",
    ")\n",
    "\n",
    "trainer2_7 = get_trainer(\"ex2_7\")\n",
    "trainer2_7.fit(model2_7, train_B_loader_216, val_B_loader_216)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 288\n",
    "embedding_dim = int(seq_length / 2)\n",
    "teacher_forcing_ratio = 0 \n",
    "\n",
    "model2_8 = AutoencoderGRU_2(\n",
    "    seq_length=seq_length,\n",
    "    input_dim=1,\n",
    "    embedding_dim=embedding_dim,\n",
    "    teacher_forcing_ratio=teacher_forcing_ratio\n",
    ")\n",
    "\n",
    "trainer2_8 = get_trainer(\"ex2_8\")\n",
    "trainer2_8.fit(model2_8, train_B_loader_288, val_B_loader_288)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 360\n",
    "embedding_dim = int(seq_length / 2)\n",
    "teacher_forcing_ratio = 0\n",
    "\n",
    "model2_9 = AutoencoderGRU_2(\n",
    "    seq_length=seq_length,\n",
    "    input_dim=1,\n",
    "    embedding_dim=embedding_dim,\n",
    "    teacher_forcing_ratio=teacher_forcing_ratio\n",
    ")\n",
    "\n",
    "trainer2_9 = get_trainer(\"ex2_9\")\n",
    "trainer2_9.fit(model2_9, train_B_loader_360, val_B_loader_360)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 432\n",
    "embedding_dim = int(seq_length / 2)\n",
    "teacher_forcing_ratio = 0\n",
    "\n",
    "model2_10 = AutoencoderGRU_2(\n",
    "    seq_length=seq_length,\n",
    "    input_dim=1,\n",
    "    embedding_dim=embedding_dim,\n",
    "    teacher_forcing_ratio=teacher_forcing_ratio\n",
    ")\n",
    "\n",
    "trainer2_10 = get_trainer(\"ex2_10\")\n",
    "trainer2_10.fit(model2_10, train_B_loader_432, val_B_loader_432)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dump loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_2_loss = [model2_1.train_losses, model2_1.val_losses,\n",
    "                     model2_2.train_losses, model2_2.val_losses,\n",
    "                     model2_3.train_losses, model2_3.val_losses,\n",
    "                     model2_4.train_losses, model2_4.val_losses,\n",
    "                     model2_5.train_losses, model2_5.val_losses,\n",
    "                     model2_6.train_losses, model2_6.val_losses,\n",
    "                     model2_7.train_losses, model2_7.val_losses,\n",
    "                     model2_8.train_losses, model2_8.val_losses,\n",
    "                     model2_9.train_losses, model2_9.val_losses,\n",
    "                     model2_10.train_losses, model2_10.val_losses]\n",
    "\n",
    "with open('experiment_2_loss.pkl', 'wb') as f:\n",
    "    pickle.dump(experiment_2_loss, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Dataset A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(model2_1.train_losses).mean())\n",
    "print(np.array(model2_1.train_losses).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_A_loader_144 = get_loader(data=test_A, batch_size=32, sub_length=144, ratio=0.75, shuffle=False)\n",
    "test_B_loader_144 = get_loader(data=test_B, batch_size=32, sub_length=144, ratio=0.75, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function, alpha is the same as during training  \n",
    "loss_fn = CombinedLoss(alpha=0.5)  \n",
    "\n",
    "# Modify the loss function to return the loss for each sample  \n",
    "class CombinedLossPerSample(nn.Module):  \n",
    "    def __init__(self, alpha=0.5):  \n",
    "        super(CombinedLossPerSample, self).__init__()  \n",
    "        self.alpha = alpha  \n",
    "        self.mse_loss = nn.MSELoss(reduction='none')  \n",
    "\n",
    "    def forward(self, y_true, y_pred):  \n",
    "        # Compute MSE loss for each sample  \n",
    "        mse = self.mse_loss(y_true, y_pred).mean(dim=[1, 2])  # Average over sequence length and feature dimensions  \n",
    "        # Compute fluctuation-sensitive loss  \n",
    "        delta_true = y_true[:, 1:] - y_true[:, :-1]  # Rate of change for true values  \n",
    "        delta_pred = y_pred[:, 1:] - y_pred[:, :-1]  # Rate of change for predicted values  \n",
    "        fluctuation_loss = ((delta_true - delta_pred) ** 2).mean(dim=[1, 2])  \n",
    "        # Compute combined loss  \n",
    "        combined_loss = self.alpha * mse + (1 - self.alpha) * fluctuation_loss  \n",
    "        return combined_loss  # Return the loss for each sample  \n",
    "\n",
    "# Use the new loss function  \n",
    "loss_fn = CombinedLossPerSample(alpha=0.5)  \n",
    "\n",
    "# %%  \n",
    "# Collect predictions, true values, and losses for the training set  \n",
    "train_preds = []  \n",
    "train_targets = []  \n",
    "train_losses = []  \n",
    "\n",
    "model2_1.eval()  # Set model to evaluation mode  \n",
    "with torch.no_grad():  \n",
    "    for batch in train_A_loader_144:  \n",
    "        x = batch[0].to(model2_1.device)  \n",
    "        pred = model2_1(x)  \n",
    "        loss = loss_fn(pred, x)  # Loss for each sample  \n",
    "        train_preds.append(pred.cpu())  \n",
    "        train_targets.append(x.cpu())  \n",
    "        train_losses.extend(loss.cpu().numpy())  # Add loss for each sample to the list  \n",
    "\n",
    "# %%  \n",
    "# Collect predictions, true values, and losses for the validation set  \n",
    "val_preds = []  \n",
    "val_targets = []  \n",
    "val_losses = []  \n",
    "\n",
    "with torch.no_grad():  \n",
    "    for batch in val_A_loader_144:  \n",
    "        x = batch[0].to(model2_1.device)  \n",
    "        pred = model2_1(x)  \n",
    "        loss = loss_fn(pred, x)  # Loss for each sample  \n",
    "        val_preds.append(pred.cpu())  \n",
    "        val_targets.append(x.cpu())  \n",
    "        val_losses.extend(loss.cpu().numpy())  \n",
    "\n",
    "# %%  \n",
    "# Calculate the average loss  \n",
    "avg_train_loss = np.mean(train_losses)  \n",
    "avg_val_loss = np.mean(val_losses)  \n",
    "\n",
    "print(f'Average training loss: {avg_train_loss:.6f}')  \n",
    "print(f'Average validation loss: {avg_val_loss:.6f}')  \n",
    "\n",
    "# %%  \n",
    "# Concatenate the predictions and true values across batches  \n",
    "train_preds = torch.cat(train_preds, dim=0)  \n",
    "train_targets = torch.cat(train_targets, dim=0)  \n",
    "\n",
    "val_preds = torch.cat(val_preds, dim=0)  \n",
    "val_targets = torch.cat(val_targets, dim=0)  \n",
    "\n",
    "# %%  \n",
    "# Compute errors  \n",
    "train_errors = (train_preds - train_targets).numpy()  \n",
    "val_errors = (val_preds - val_targets).numpy()  \n",
    "\n",
    "# Flatten errors to a one-dimensional array  \n",
    "train_errors = train_errors.flatten()  \n",
    "val_errors = val_errors.flatten()  \n",
    "\n",
    "# %%  \n",
    "# Plot error distribution  \n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns  \n",
    "\n",
    "plt.figure(figsize=(10, 5))  \n",
    "sns.histplot(train_errors, bins=100, kde=True, color='blue', label='Training Error')  \n",
    "sns.histplot(val_errors, bins=100, kde=True, color='red', label='Validation Error')  \n",
    "plt.legend()  \n",
    "plt.title('Error Distribution')  \n",
    "plt.xlabel('Error')  \n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('Error Distribution.svg', dpi=300) \n",
    "plt.show()  \n",
    "\n",
    "# %%  \n",
    "# Plot loss distribution  \n",
    "plt.figure(figsize=(10, 5))  \n",
    "sns.histplot(train_losses, bins=100, kde=True, color='blue', label='Training Loss')  \n",
    "sns.histplot(val_losses, bins=100, kde=True, color='red', label='Validation Loss')  \n",
    "plt.legend()  \n",
    "plt.title('Loss Distribution')  \n",
    "plt.xlabel('Loss')  \n",
    "plt.ylabel('Frequency')  \n",
    "plt.savefig('Loss Distribution.svg', dpi=300) \n",
    "plt.show()  \n",
    "\n",
    "# %%  \n",
    "# Calculate the mean and standard deviation of predictions for the training and validation sets  \n",
    "train_mean = train_preds.mean().item()  \n",
    "train_std = train_preds.std().item()  \n",
    "\n",
    "val_mean = val_preds.mean().item()  \n",
    "val_std = val_preds.std().item()  \n",
    "\n",
    "print(f'Mean of training predictions: {train_mean:.4f}, Standard deviation: {train_std:.4f}')  \n",
    "print(f'Mean of validation predictions: {val_mean:.4f}, Standard deviation: {val_std:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "import torch  \n",
    "\n",
    "# Define validation set mean and std dev of the loss  \n",
    "mu_epsilon = 0.009993348002899438 \n",
    "sigma_epsilon = 0.007432170408685299 \n",
    "\n",
    "# Collect predictions, true values, and losses for the test set  \n",
    "test_preds = []  \n",
    "test_targets = []  \n",
    "\n",
    "# Initialize the loss function, alpha is the same as during training  \n",
    "loss_fn = CombinedLoss(alpha=0.5)  \n",
    "\n",
    "model2_1.eval()  # Set model to evaluation mode  \n",
    "with torch.no_grad():  \n",
    "    for batch in test_A_loader_144:  \n",
    "        x = batch[0].to(model2_1.device)  \n",
    "        pred = model2_1(x)  \n",
    "        test_preds.append(pred.cpu())  \n",
    "        test_targets.append(x.cpu())  \n",
    "\n",
    "# Concatenate predictions and true values across batches  \n",
    "test_preds = torch.cat(test_preds, dim=0)  \n",
    "test_targets = torch.cat(test_targets, dim=0)  \n",
    "\n",
    "# Calculate per-time-point losses (using the same loss function as for training)  \n",
    "errors = loss_fn(test_targets, test_preds).numpy()  \n",
    "\n",
    "# Calculate anomaly probabilities for each time point  \n",
    "anomaly_probabilities =  1 - np.exp(-0.5 * ((errors - mu_epsilon) / sigma_epsilon) ** 2) \n",
    "\n",
    "# Select the number of samples to plot  \n",
    "num_examples = 5  # You can adjust this number as needed  \n",
    "\n",
    "# Function to find contiguous anomaly regions\n",
    "def find_anomaly_regions(anomaly_indices):\n",
    "    if len(anomaly_indices) == 0:\n",
    "        return []\n",
    "    regions = []\n",
    "    start = anomaly_indices[0]\n",
    "    end = anomaly_indices[0]\n",
    "    for idx in anomaly_indices[1:]:\n",
    "        if idx == end + 1:\n",
    "            end = idx\n",
    "        else:\n",
    "            regions.append((start, end))\n",
    "            start = idx\n",
    "            end = idx\n",
    "    regions.append((start, end))\n",
    "    return regions\n",
    "\n",
    "def plot_result_A(i=1):\n",
    "    plt.figure(figsize=(15, 10))  \n",
    "\n",
    "    # Extract the i-th sample's input (true values), output (predicted values), and anomaly probabilities  \n",
    "    input_seq = test_targets[i].numpy().flatten()  \n",
    "    output_seq = test_preds[i].numpy().flatten()  \n",
    "    anomaly_seq = anomaly_probabilities[i].flatten()  # Flatten the anomaly probabilities sequence  \n",
    "\n",
    "    # Identify anomaly positions where anomaly probability > 0.8  \n",
    "    anomaly_threshold = 0.8  \n",
    "    anomaly_indices = np.where(anomaly_seq > anomaly_threshold)[0]  \n",
    "    anomaly_regions = find_anomaly_regions(anomaly_indices)\n",
    "\n",
    "    # Create a subplot structure with three rows and one column\n",
    "    plt.subplot(3, 1, 1)  \n",
    "    plt.plot(input_seq, label='Input Sequence', color='blue')  \n",
    "    # Highlight anomaly regions on the input sequence  \n",
    "    for start, end in anomaly_regions:\n",
    "        plt.axvspan(start, end, color='red', alpha=0.3)\n",
    "    plt.title(f'Test Sample {i+1} - Input Sequence')  \n",
    "    plt.xlabel('Time Step')  \n",
    "    plt.ylabel('Value')  \n",
    "    plt.legend()  \n",
    "\n",
    "    plt.subplot(3, 1, 2)  \n",
    "    plt.plot(output_seq, label='Output Sequence', color='red')  \n",
    "    plt.title(f'Test Sample {i+1} - Output Sequence')  \n",
    "    plt.xlabel('Time Step')  \n",
    "    plt.ylabel('Value')  \n",
    "    plt.legend()  \n",
    "\n",
    "    plt.subplot(3, 1, 3)  \n",
    "    plt.plot(anomaly_seq, label='Anomaly Probability', color='green')  \n",
    "    # Plot the anomaly probability threshold line  \n",
    "    plt.axhline(y=anomaly_threshold, color='orange', linestyle='--', label='Threshold (0.8)')  \n",
    "    # Highlight anomaly probabilities above the threshold  \n",
    "    if len(anomaly_indices) > 0:  \n",
    "        plt.scatter(anomaly_indices, anomaly_seq[anomaly_indices], color='red', label='Anomaly (>0.8)')  \n",
    "    plt.title(f'Test Sample {i+1} - Anomaly Probability')  \n",
    "    plt.xlabel('Time Step')  \n",
    "    plt.ylabel('Probability')  \n",
    "    plt.legend()  \n",
    "\n",
    "    plt.tight_layout()  \n",
    "    plt.savefig(f'Test Sample {i+1} - Anomaly Probability.svg', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result_A(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result_A(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Dataset B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path_B = \"checkpoint/ex2_1/epoch=92-val_loss=0.01.ckpt\"\n",
    "model_B = AutoencoderGRU_2.load_from_checkpoint(checkpoint_path_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error(ts1, ts2, error_limit):\n",
    "    error = (ts1 - ts2)**2\n",
    "\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "    axs[0].plot(ts1, label='Input')\n",
    "    axs[0].plot(ts2, label='Output', linestyle='--')\n",
    "    axs[0].set_title('Input and output comparison')\n",
    "    \n",
    "    over_tolerance = np.abs(error) > error_limit\n",
    "    axs[0].scatter(np.where(over_tolerance), ts1[over_tolerance], color='red', label='Out of Tolerance (anormaly)')\n",
    "    # axs[0].scatter(np.where(over_tolerance), ts2[over_tolerance], color='yellow', label='Out of Tolerance (TS2)')\n",
    "    axs[0].legend(loc='upper right')\n",
    "\n",
    "    axs[1].plot(error, color='red', label='Squared Error')\n",
    "    axs[1].set_title('Reconstruction error')\n",
    "    axs[1].axhline(y=error_limit, color='green', linestyle='--', label='Tolerance Threshold')\n",
    "    axs[1].set_xlabel('Time Step')\n",
    "    axs[1].set_ylabel('Squared Error')\n",
    "    axs[1].legend(loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_A_loader_288 = get_loader(data=test_A, batch_size=32, sub_length=288, ratio=0.75, shuffle=False)\n",
    "test_B_loader_288 = get_loader(data=test_B, batch_size=32, sub_length=288, ratio=0.75, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_length=288\n",
    "# embedding_dim = int(seq_length / 2) \n",
    "# teacher_forcing_ratio = 0\n",
    "\n",
    "# checkpoint_path_1_1 = \"checkpoint/ex1_1/epoch=89-val_loss=0.01.ckpt\"\n",
    "# model_1_1 = AutoencoderGRU_2.load_from_checkpoint(\n",
    "#     checkpoint_path_1_1,\n",
    "#     seq_length=seq_length,\n",
    "#     input_dim=1,\n",
    "#     embedding_dim=embedding_dim,\n",
    "#     teacher_forcing_ratio=teacher_forcing_ratio\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer()\n",
    "\n",
    "predictions = trainer.predict(model1_1, dataloaders=test_A_loader_288)\n",
    "\n",
    "predicted_values = [pred.detach().cpu().numpy() for pred in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_in_out(batch, sample):\n",
    "    for i, data in enumerate(test_A_loader_288):\n",
    "        if i == batch:\n",
    "            x = data[0]\n",
    "            x = x[sample]\n",
    "            break\n",
    "\n",
    "    pred = predicted_values[batch][sample]\n",
    "\n",
    "    print(x.shape)\n",
    "    print(pred.shape)\n",
    "    plot_error(x.numpy(), pred, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_in_out(2,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_1.train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_A_loader_144 = get_loader(data=test_A, batch_size=32, sub_length=144, ratio=0.75, shuffle=False)\n",
    "test_B_loader_144 = get_loader(data=test_B, batch_size=32, sub_length=144, ratio=0.75, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length=144\n",
    "embedding_dim = int(seq_length / 2)\n",
    "teacher_forcing_ratio = 0 \n",
    "\n",
    "checkpoint_path_2_1 = \"checkpoint/ex2_1/epoch=93-val_loss=0.01.ckpt\"\n",
    "model_2_1 = AutoencoderGRU_2.load_from_checkpoint(\n",
    "    checkpoint_path_2_1,\n",
    "    seq_length=seq_length,\n",
    "    input_dim=1,\n",
    "    embedding_dim=embedding_dim,\n",
    "    teacher_forcing_ratio=teacher_forcing_ratio,\n",
    "    alpha=0.2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer()\n",
    "\n",
    "predictions = trainer.predict(model2_1, dataloaders=test_A_loader_144)\n",
    "\n",
    "predicted_values = [pred.detach().cpu().numpy() for pred in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_in_out(batch, sample):\n",
    "    for i, data in enumerate(test_A_loader_144):\n",
    "        if i == batch:\n",
    "            x = data[0]\n",
    "            x = x[sample]\n",
    "            break\n",
    "\n",
    "    pred = predicted_values[batch][sample]\n",
    "\n",
    "    print(x.shape)\n",
    "    print(pred.shape)\n",
    "    plot_error(x.numpy(), pred, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_in_out(1, 18)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
